{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from ridgereliability import utils\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.integrate\n",
    "\n",
    "import ridgereliability.beta\n",
    "import sklearn.metrics\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.svm\n",
    "import sklearn.preprocessing\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic classification: toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.make_classification(n_samples=5000, n_features=5, n_informative=3, n_classes=3)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.svm.SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n",
      "Balanced accuracy: 0.8682246668791995\n"
     ]
    }
   ],
   "source": [
    "y_probs = logreg.predict_proba(X_test)\n",
    "y_preds = y_probs.argmax(axis=1)\n",
    "\n",
    "print(f\"Accuracy: {sklearn.metrics.accuracy_score(y_test, y_preds)}\")\n",
    "print(f\"Balanced accuracy: {sklearn.metrics.balanced_accuracy_score(y_test, y_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def peace(y_probs, y_preds, y_true, samples=1000, bins=\"fd\"):\n",
    "    \"\"\"Compute the posterior expected balanced accuracy-based calibration error (PEACE).\n",
    "\n",
    "    Parameters:\n",
    "    y_probs (np.array): predicted class probabilities\n",
    "    y_preds (np.array): predicted class labels\n",
    "    y_true (np.array): true class labels\n",
    "    samples (int): number of samples for numerical integration\n",
    "\n",
    "    Returns:\n",
    "    exp_ce (float): expected calibration error\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # define the bin function\n",
    "    def bin_func(y_probs_bin, y_preds_bin, y_true_bin):\n",
    "        \n",
    "        xs = np.linspace(0, 1, samples)\n",
    "        conf = y_probs_bin.mean()\n",
    "        \n",
    "        if len(np.unique(y_preds_bin)) > 1:\n",
    "            # estimate beta parameters\n",
    "            confusion = sklearn.metrics.confusion_matrix(y_true_bin, y_preds_bin)\n",
    "            params = ridgereliability.beta.get_beta_parameters(confusion)\n",
    "            ys = abs(xs - conf) * ridgereliability.beta.beta_avg_pdf(xs, params, fft=True)\n",
    "        else:\n",
    "            params = sum(y_preds_bin == y_true_bin)+1, sum(y_preds_bin != y_true_bin)+1\n",
    "            ys = abs(xs - conf) * scipy.stats.beta.pdf(xs, params[0], params[1])\n",
    "            \n",
    "        # approximate the integral using Simpson's rule\n",
    "        return scipy.integrate.simps(ys, xs)\n",
    "\n",
    "    # compute the full result\n",
    "    bin_indices = utils.get_bin_indices(y_probs, bins=bins)\n",
    "    return utils.binning(y_probs, y_preds, y_true, bin_indices, bin_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the metric with the probabilities associated with the prediction, the prediction itself, and the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05083328786600578"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peace(y_probs.max(axis=1), y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def ece(y_probs, y_preds, y_true, balanced=False, bins=\"fd\"):\n",
    "    \"\"\"Compute the expected calibration error (ECE).\n",
    "\n",
    "    Parameters:\n",
    "    y_probs (np.array): predicted class probabilities\n",
    "    y_preds (np.array): predicted class labels\n",
    "    y_true (np.array): true class labels\n",
    "\n",
    "    Returns:\n",
    "    exp_ce (float): expected calibration error\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # define the bin function\n",
    "    def bin_func(y_probs_bin, y_preds_bin, y_true_bin):\n",
    "        acc = (y_preds_bin == y_true_bin).mean()\n",
    "        conf = y_probs_bin.mean()\n",
    "        return abs(acc - conf)\n",
    "\n",
    "    # define the balanced bin function\n",
    "    def balanced_bin_func(y_probs_bin, y_preds_bin, y_true_bin):\n",
    "        balacc = sklearn.metrics.balanced_accuracy_score(y_true_bin, y_preds_bin)\n",
    "        conf = y_probs_bin.mean()\n",
    "        return abs(balacc - conf)\n",
    "\n",
    "    # compute the full result\n",
    "    bin_indices = utils.get_bin_indices(y_probs, bins=bins)\n",
    "    func = balanced_bin_func if balanced else bin_func\n",
    "    return utils.binning(y_probs, y_preds, y_true, bin_indices, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04008998620662349"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ece(y_probs.max(axis=1), y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def ece_v2(y_probs, y_preds, y_true, bins=\"fd\"):\n",
    "    \"\"\"Compute the expected calibration error based on the expected posterior balanced accuracy (ECEv2).\n",
    "\n",
    "    Parameters:\n",
    "    y_probs (np.array): predicted class probabilities\n",
    "    y_preds (np.array): predicted class labels\n",
    "    y_true (np.array): true class labels\n",
    "\n",
    "    Returns:\n",
    "    exp_ce (float): expected calibration error\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # define the bin function\n",
    "    def bin_func(y_probs_bin, y_preds_bin, y_true_bin):\n",
    "        confusion = sklearn.metrics.confusion_matrix(y_true_bin, y_preds_bin)\n",
    "        acc = ridgereliability.beta.balanced_accuracy_expected(confusion, fft=True)\n",
    "        conf = y_probs_bin.mean()\n",
    "        return abs(acc - conf)\n",
    "\n",
    "    # compute the full result\n",
    "    bin_indices = utils.get_bin_indices(y_probs, bins=bins)\n",
    "    return utils.binning(y_probs, y_preds, y_true, bin_indices, bin_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03654042284862339"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ece_v2(y_probs.max(axis=1), y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def class_wise_error(y_probs, y_preds, y_true, base_error, *base_error_args, **base_error_kwargs):\n",
    "    \"\"\"Compute classwise-error as proposed in \"Beyond temperature scaling: Obtaining well-calibrated\n",
    "    multiclass probabilities with Dirichlet calibration\" (Kull, 2019).\n",
    "\n",
    "    Parameters:\n",
    "    y_probs (np.array): predicted class probabilities\n",
    "    y_preds (np.array): predicted class labels\n",
    "    y_true (np.array): true class labels\n",
    "    base_error (callable): function that returns ECE for given probabilities, label predictions and true labels\n",
    "    base_error_[kw]args ([kw]args): [Keyword ]arguments that should be passed to the base_ece callable.\n",
    "\n",
    "    Returns:\n",
    "    exp_ce (float): class-wise expected calibration error\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = sklearn.utils.multiclass.unique_labels(y_preds, y_true)\n",
    "    y_true_binarized = np.eye(np.max(classes) + 1, dtype=int)[y_true]\n",
    "    y_preds_binarized = np.eye(np.max(classes) + 1, dtype=int)[y_preds]\n",
    "\n",
    "    result = 0.\n",
    "    for c in classes:\n",
    "        selector = y_preds == c\n",
    "\n",
    "        result += base_error(y_probs[selector, c], y_preds[selector], y_true[selector], *base_error_args, **base_error_kwargs)\n",
    "\n",
    "    return result/len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07013434462821751"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_wise_error(y_probs, y_preds, y_test, base_error=peace, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
