# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/metrics.ipynb (unless otherwise specified).

__all__ = ['peace', 'ece', 'ece_v2', 'class_wise_error']

# Cell

from ridgereliability import utils

import numpy as np
import scipy.stats
import scipy.integrate

import ridgereliability.beta
import sklearn.metrics
import sklearn.datasets
import sklearn.model_selection
import sklearn.svm
import sklearn.preprocessing
import sklearn.utils

# Cell

def peace(y_probs, y_preds, y_true, samples=1000, bins="fd"):
    """Compute the posterior expected balanced accuracy-based calibration error (PEACE).

    Parameters:
    y_probs (np.array): predicted class probabilities
    y_preds (np.array): predicted class labels
    y_true (np.array): true class labels
    samples (int): number of samples for numerical integration

    Returns:
    exp_ce (float): expected calibration error

    """

    # define the bin function
    def bin_func(y_probs_bin, y_preds_bin, y_true_bin):
        # estimate beta parameters
        confusion = sklearn.metrics.confusion_matrix(y_true_bin, y_preds_bin)
        params = ridgereliability.beta.get_beta_parameters(confusion)

        # approximate the integral using Simpson's rule
        xs = np.linspace(0, 1, samples)
        conf = y_probs_bin.mean()
        ys = abs(xs - conf) * ridgereliability.beta.beta_avg_pdf(xs, params, fft=True)
        return scipy.integrate.simps(ys, xs)

    # compute the full result
    bin_indices = utils.get_bin_indices(y_probs, bins=bins)
    return utils.binning(y_probs, y_preds, y_true, bin_indices, bin_func)

# Cell

def ece(y_probs, y_preds, y_true, balanced=False, bins="fd"):
    """Compute the expected calibration error (ECE).

    Parameters:
    y_probs (np.array): predicted class probabilities
    y_preds (np.array): predicted class labels
    y_true (np.array): true class labels

    Returns:
    exp_ce (float): expected calibration error

    """

    # define the bin function
    def bin_func(y_probs_bin, y_preds_bin, y_true_bin):
        acc = (y_preds_bin == y_true_bin).mean()
        conf = y_probs_bin.mean()
        return abs(acc - conf)

    # define the balanced bin function
    def balanced_bin_func(y_probs_bin, y_preds_bin, y_true_bin):
        balacc = sklearn.metrics.balanced_accuracy_score(y_true_bin, y_preds_bin)
        conf = y_probs_bin.mean()
        return abs(balacc - conf)

    # compute the full result
    bin_indices = utils.get_bin_indices(y_probs, bins=bins)
    func = balanced_bin_func if balanced else bin_func
    return utils.binning(y_probs, y_preds, y_true, bin_indices, func)

# Cell

def ece_v2(y_probs, y_preds, y_true, bins="fd"):
    """Compute the expected calibration error based on the expected posterior balanced accuracy (ECEv2).

    Parameters:
    y_probs (np.array): predicted class probabilities
    y_preds (np.array): predicted class labels
    y_true (np.array): true class labels

    Returns:
    exp_ce (float): expected calibration error

    """

    # define the bin function
    def bin_func(y_probs_bin, y_preds_bin, y_true_bin):
        confusion = sklearn.metrics.confusion_matrix(y_true_bin, y_preds_bin)
        acc = ridgereliability.beta.balanced_accuracy_expected(confusion, fft=True)
        conf = y_probs_bin.mean()
        return abs(acc - conf)

    # compute the full result
    bin_indices = utils.get_bin_indices(y_probs, bins=bins)
    return utils.binning(y_probs, y_preds, y_true, bin_indices, bin_func)

# Cell

def class_wise_error(y_probs, y_preds, y_true, base_error, *base_error_args, **base_error_kwargs):
    """Compute classwise-error as proposed in "Beyond temperature scaling: Obtaining well-calibrated
    multiclass probabilities with Dirichlet calibration" (Kull, 2019).

    Parameters:
    y_probs (np.array): predicted class probabilities
    y_preds (np.array): predicted class labels
    y_true (np.array): true class labels
    base_error (callable): function that returns ECE for given probabilities, label predictions and true labels
    base_error_[kw]args ([kw]args): [Keyword ]arguments that should be passed to the base_ece callable.

    Returns:
    exp_ce (float): class-wise expected calibration error

    """

    classes = sklearn.utils.multiclass.unique_labels(y_preds, y_true)
    y_true_binarized = np.eye(np.max(classes) + 1, dtype=int)[y_true]
    y_preds_binarized = np.eye(np.max(classes) + 1, dtype=int)[y_preds]

    result = 0.
    for c in classes:
        probs = np.where(y_preds_binarized[:, c]==0, 1-y_probs[:, c], y_probs[:, c])
        result += base_error(probs, y_preds_binarized[:, c], y_true_binarized[:, c], *base_error_args, **base_error_kwargs)

    return result/len(classes)