{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook compare_metrics_over_datasets.ipynb to python\n",
      "[NbConvertApp] Writing 7941 bytes to compare_metrics_over_datasets.py\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "!jupyter nbconvert --RegexRemovePreprocessor.patterns=\"['# ?noscript']\" --TemplateExporter.exclude_markdown=True --to python compare_metrics_over_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import openml\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.base\n",
    "import sklearn.utils\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.neural_network\n",
    "import sklearn.tree\n",
    "\n",
    "from ridgereliability import plots, metrics\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "import logging\n",
    "import time\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_ipython().__class__.__name__ != 'ZMQInteractiveShell':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n-processes\", type=int, required=True)\n",
    "    n_procs = parser.parse_args().n_processes\n",
    "else:\n",
    "    n_procs = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare metrics over datasets\n",
    "\n",
    "In this notebook we collect calibration estimates for several models and datasets. The estimates are computed with the Expected Calibration Error (ECE), Balanced-ECE, and Posterior Expected Accuracy-based Calibration Error (PEACE).\n",
    "\n",
    "Tested models:\n",
    "- Random Forest\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Gaussian Naive Bayes\n",
    "- MLP\n",
    "- AdaBoost\n",
    "- Decision Tree\n",
    "\n",
    "Tested datasets (from Open ML):\n",
    "- [eeg-eye-state](https://www.openml.org/d/1471), task [9983](https://www.openml.org/t/9983)\n",
    "- [Phoneme](https://www.openml.org/d/1489), task [9952](https://www.openml.org/t/9952)\n",
    "- [mozilla4](https://www.openml.org/d/1046), task [3899](https://www.openml.org/t/3899)\n",
    "- [electricity](https://www.openml.org/d/151), task [219](https://www.openml.org/t/219)\n",
    "- [Magic Telescope](https://www.openml.org/d/1120), task [3954](https://www.openml.org/t/3954)\n",
    "- [artificial characters](https://www.openml.org/d/1459), task [14964](https://www.openml.org/t/14964)\n",
    "- [pendigits](https://www.openml.org/d/32), task [32](https://www.openml.org/t/32)\n",
    "- [letter](https://www.openml.org/d/6), task [6](https://www.openml.org/t/6)\n",
    "- [JapaneseVowels](https://www.openml.org/d/375), task [3510](https://www.openml.org/t/3510)\n",
    "- [glass](https://www.openml.org/d/41), task [40](https://www.openml.org/t/40)\n",
    "- [micro-mass](https://www.openml.org/d/1515), task [9950](https://www.openml.org/t/9950)\n",
    "- [vehicle](https://www.openml.org/d/54), task [53](https://www.openml.org/t/53)\n",
    "- [synthetic-control](https://www.openml.org/d/377), task [3512](https://www.openml.org/t/3512)\n",
    "- [mfeat-factors](https://www.openml.org/d/12), task [12](https://www.openml.org/t/12)\n",
    "- [OVA_breast](https://www.openml.org/d/1128), task [3962](https://www.openml.org/d/1128)\n",
    "- [sonar](https://www.openml.org/d/40), task [39](https://www.openml.org/t/39)\n",
    "- [visualizing_livestock](https://www.openml.org/d/685), task [3577](https://www.openml.org/t/3577)\n",
    "- [spectrometer](https://www.openml.org/d/313), task [145682](https://www.openml.org/t/145682)\n",
    "- [disclosure_z](https://www.openml.org/d/931), task [3794](https://www.openml.org/t/3794)\n",
    "- [mfeat-pixel](https://www.openml.org/d/40979), task [146824](https://www.openml.org/t/146824)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "First of all, we need to be able to load and preprocess multiple datasets appropriately to prepare it for classification. \n",
    "To this end we implement a function that will load a dataset and classification task from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [9983, 9952, 3899, 219, 3954, 14964, 32, 6, 3510, 40, 9950, 53, 3512, 12, 3962, 39, 3577, 145682, 3794, 146824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openml_task(task_id):\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    X, y = task.get_X_and_y(\"dataframe\")\n",
    "    \n",
    "    X_enc = sklearn.preprocessing.OrdinalEncoder().fit_transform(X.values)\n",
    "    X = pandas.DataFrame(X_enc, columns=X.columns, index=X.index)\n",
    "    \n",
    "    n_repeats, n_folds, n_samples = task.get_split_dimensions()\n",
    "\n",
    "    folds = numpy.empty((len(X)), dtype=int)\n",
    "    for fold_idx in range(n_folds):\n",
    "        _, test_indices = task.get_train_test_split_indices(\n",
    "            repeat=0,\n",
    "            fold=fold_idx,\n",
    "            sample=0,\n",
    "        )\n",
    "        \n",
    "        folds[test_indices] = fold_idx\n",
    "        \n",
    "    splitter = sklearn.model_selection.PredefinedSplit(folds)\n",
    "                \n",
    "    return X, y, splitter        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: 2020-09-24 17:09:19,682 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:19,686 - Data pickle file already exists and is up to date.\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "X, y, splitter = load_openml_task(9952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(541,)\n",
      "(541,)\n",
      "(541,)\n",
      "(541,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "for train_idx, test_idx in splitter.split():\n",
    "    print(test_idx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting calibration metrics\n",
    "\n",
    "For each dataset we fit and evaluate multiple models. We record calibration metrics (ECE, ECE-balanced, PEACE), and some performance metrics (balanced accuracy, F1-score, accuracy). To this end we implement a function that runs this procedure for one model on one OpenML task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"rf\": sklearn.ensemble.RandomForestClassifier(),\n",
    "    \"svm\": sklearn.svm.SVC(probability=True),\n",
    "    \"logreg\": sklearn.linear_model.LogisticRegression(max_iter=500),\n",
    "    \"nb\": sklearn.naive_bayes.GaussianNB(),\n",
    "    \"mlp\": sklearn.neural_network.MLPClassifier(max_iter=500),\n",
    "    \"adaboost\": sklearn.ensemble.AdaBoostClassifier(n_estimators=500),\n",
    "    \"dectree\": sklearn.tree.DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_metrics_for_model(row, Xt, yt, Xv, yv):\n",
    "    # get and fit fresh model\n",
    "    model = sklearn.base.clone(MODELS[row[\"model_id\"]])\n",
    "    model.fit(Xt, yt)\n",
    "\n",
    "    # compute metrics on test data\n",
    "    y_probs = model.predict_proba(Xv)\n",
    "    y_probs_max = y_probs.max(axis=1)\n",
    "    y_preds = model.predict(Xv)\n",
    "    y_test = yv\n",
    "\n",
    "    bins = 15\n",
    "    row.update({\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(y_test, y_preds),\n",
    "        \"balanced_accuracy\": sklearn.metrics.balanced_accuracy_score(y_test, y_preds),\n",
    "        \"f1\": sklearn.metrics.f1_score(y_test, y_preds, average=\"weighted\"),\n",
    "        'ece': metrics.ece(y_probs_max, y_preds, y_test, bins=bins),\n",
    "        'ece_balanced': metrics.ece(y_probs_max, y_preds, y_test, balanced=True, bins=bins),\n",
    "        'peace': metrics.peace(y_probs_max, y_preds, y_test, bins=bins),\n",
    "        'class_wise_ece': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.ece, bins=bins),\n",
    "        'class_wise_peace': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.peace, bins=bins)\n",
    "    })\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_metrics_for_model_and_task(model_id, task_id, pool, n_repeats, counter, start_at):\n",
    "    \n",
    "    X, y, splitter = load_openml_task(task_id) # repeated runs will use cached data\n",
    "    \n",
    "    promises = []\n",
    "    for i, (train_idx, test_idx) in enumerate(splitter.split()):\n",
    "        for j in range(n_repeats):\n",
    "            counter += 1\n",
    "            if counter < start_at:\n",
    "                continue\n",
    "            \n",
    "            row = {\n",
    "                \"fold\": i,\n",
    "                \"repeat\": j,\n",
    "                \"model_id\": model_id,\n",
    "                \"task_id\": task_id,\n",
    "            }\n",
    "\n",
    "            # split data\n",
    "            Xt, yt = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            Xv, yv = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "            promise = pool.apply_async(\n",
    "                get_fold_metrics_for_model,\n",
    "                (row, Xt, yt, Xv, yv)\n",
    "            )\n",
    "            promises.append(promise)\n",
    "        \n",
    "    return promises, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection over all datasets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2020-09-24 17:09:19,774 - Output to metrics_1600960159.dat\n",
      "DEBUG: 2020-09-24 17:09:19,780 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:19,786 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:19,850 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:19,864 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:19,925 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:19,935 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:19,992 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,012 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,345 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,356 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 17:09:20,359 - Going to remove the following attributes: ['ID']\n",
      "DEBUG: 2020-09-24 17:09:20,478 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,488 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,539 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,550 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,632 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,647 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,752 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,765 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,849 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:20,858 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:21,008 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:21,165 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:21,511 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:21,521 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:21,565 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:21,623 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 17:09:21,647 - Going to remove the following attributes: ['index']\n",
      "DEBUG: 2020-09-24 17:09:21,782 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:21,809 - Data pickle file already exists and is up to date.\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "DEBUG: 2020-09-24 17:09:23,437 - Data pickle file already exists and is up to date.\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "DEBUG: 2020-09-24 17:09:24,886 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 17:09:24,981 - Going to remove the following attributes: ['ID_REF']\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "DEBUG: 2020-09-24 17:09:34,123 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,136 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,176 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,184 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,227 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,251 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 17:09:34,255 - Going to remove the following attributes: ['LRS-name']\n",
      "DEBUG: 2020-09-24 17:09:34,358 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,366 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,416 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,446 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,588 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,600 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,687 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,696 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,738 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,749 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,816 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:34,850 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:35,362 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:35,375 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 17:09:35,378 - Going to remove the following attributes: ['ID']\n",
      "DEBUG: 2020-09-24 17:09:35,654 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:35,665 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:35,794 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:35,806 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:35,930 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:35,944 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:36,067 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:36,078 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:36,170 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:36,191 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:09:36,466 - Data pickle file already exists and is up to date.\n",
      "Process ForkPoolWorker-7:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/xmltodict.py\u001b[0m in \u001b[0;36mpush_data\u001b[0;34m(self, item, key, data)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'oml:data_type'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-13e809f10802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTASKS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cv_metrics_for_model_and_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mpromises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c4debd87ea98>\u001b[0m in \u001b[0;36mget_cv_metrics_for_model_and_task\u001b[0;34m(model_id, task_id, pool, n_repeats, counter, start_at)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_cv_metrics_for_model_and_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_openml_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# repeated runs will use cached data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpromises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-732ad3844104>\u001b[0m in \u001b[0;36mload_openml_task\u001b[0;34m(task_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_openml_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_X_and_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataframe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/openml/tasks/task.py\u001b[0m in \u001b[0;36mget_X_and_y\u001b[0;34m(self, dataset_format)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_type_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/openml/tasks/task.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenMLDataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;34m\"\"\"Download dataset associated with task\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     def get_train_test_split_indices(\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(dataset_id, download_data, version, error_if_multiple)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mremove_dataset_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dataset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdid_cache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dataset_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdid_cache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36m_get_dataset_features\u001b[0;34m(did_cache_dir, dataset_id)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_xml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_load_features_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36m_load_features_from_file\u001b[0;34m(features_file)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mfeatures_xml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         xml_dict = xmltodict.parse(features_xml,\n\u001b[0m\u001b[1;32m    347\u001b[0m                                    force_list=('oml:feature', 'oml:nominal_value'))\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxml_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"oml:data_features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/xmltodict.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(xml_input, encoding, expat, process_namespaces, namespace_separator, disable_entities, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/build/80754af9/python_1585235154784/work/Modules/pyexpat.c\u001b[0m in \u001b[0;36mEndElement\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/xmltodict.py\u001b[0m in \u001b[0;36mendElement\u001b[0;34m(self, full_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/xmltodict.py\u001b[0m in \u001b[0;36mpush_data\u001b[0;34m(self, item, key, data)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_force_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/xmltodict.py\u001b[0m in \u001b[0;36m_should_force_list\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_should_force_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool(processes=n_procs) as pool:\n",
    "    \n",
    "    start_at = 0\n",
    "    \n",
    "    output_file = f\"metrics_{int(time.time())}.dat\"\n",
    "    logging.info(f\"Output to {output_file}\")\n",
    "    \n",
    "    promises = []\n",
    "    counter = 0\n",
    "    for model_id in MODELS.keys():\n",
    "        for task_id in TASKS:\n",
    "            tmp, counter = get_cv_metrics_for_model_and_task(model_id, task_id, pool, 1, counter, start_at)\n",
    "            promises.extend(tmp)\n",
    "            \n",
    "    logging.info(f\"{len(promises)} promises submitted to pool\")\n",
    "            \n",
    "    data = []\n",
    "    for promise in promises:\n",
    "        data.append(promise.get())\n",
    "        logging.info(f\"Finished promises: {len(data)}/{len(promises)} ({len(data)/len(promises)*100:.2f}%)\")\n",
    "        df = pandas.DataFrame(data)\n",
    "        dump(df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_ipython().__class__.__name__ != 'ZMQInteractiveShell':\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load(\"/home/maximl/Data/Experiment_data/results/riverrel/metrics_1600940535.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load(\"metrics_1600957213.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby([\"model_id\", \"task_id\", \"repeat\"]).aggregate(\"mean\").drop(columns=[\"fold\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longform(df, cols=None, subject_cols=None):\n",
    "    dfs = []\n",
    "    \n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    \n",
    "    for col in cols:\n",
    "        tmp_df = pandas.DataFrame(dict(        \n",
    "            value=df[col], \n",
    "            metric=col,   \n",
    "        ))\n",
    "        for col2 in set(df.columns) - set(cols):\n",
    "            tmp_df[col2] = df[col2]\n",
    "            \n",
    "        if subject_cols is not None:\n",
    "            tmp_df[\"subject\"] = df[subject_cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "            \n",
    "        dfs.append(tmp_df)\n",
    "        \n",
    "    return pandas.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = get_longform(grouped_df, grouped_df.columns[3:], [\"model_id\", \"task_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin([\"accuracy\", \"balanced_accuracy\", \"f1\"])], x=\"model_id\", y=\"value\", col=\"metric\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin([\"accuracy\", \"balanced_accuracy\", \"f1\"])], x=\"task_id\", y=\"value\", col=\"metric\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.displot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], x=\"value\", col=\"metric\", rug=True, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.boxplot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], y=\"value\", x=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lineplot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], y=\"value\", x=\"metric\", hue=\"subject\", err_style=\"bars\", palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled datasets + model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = grouped_df.loc[grouped_df[\"repeat\"] == 0, [\"ece\", \"ece_balanced\", \"peace\"]].values\n",
    "scipy.stats.friedmanchisquare(data[0], data[1], data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_data = get_longform(grouped_df.loc[grouped_df[\"repeat\"] == 0, [\"ece\", \"ece_balanced\", \"peace\"]])\n",
    "sp.posthoc_conover(long_data, val_col=\"value\", group_col=\"metric\", p_adjust=\"holm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model_df in grouped_df.groupby(\"model_id\"):\n",
    "    data = model_df.loc[:, [\"ece\", \"ece_balanced\", \"peace\"]]\n",
    "    test = scipy.stats.friedmanchisquare(data.iloc[:, 0], data.iloc[:, 1], data.iloc[:, 2])\n",
    "    print(idx)\n",
    "    print(test)\n",
    "    if test.pvalue < 0.05:\n",
    "        long_data = get_longform(data)\n",
    "        print(sp.posthoc_conover(long_data, val_col=\"value\", group_col=\"metric\", p_adjust=\"holm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], x=\"metric\", y=\"value\", col=\"model_id\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
