{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook compare_metrics_over_datasets.ipynb to python\n",
      "[NbConvertApp] Writing 20599 bytes to adadecsvm_large.py\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "!jupyter nbconvert --RegexRemovePreprocessor.patterns=\"['# ?noscript']\" --TemplateExporter.exclude_markdown=True --to python compare_metrics_over_datasets.ipynb --output adadecsvm_large.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate\n",
    "\n",
    "import openml\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.base\n",
    "import sklearn.utils\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.neural_network\n",
    "import sklearn.tree\n",
    "import xgboost\n",
    "\n",
    "from ridgereliability import plots, metrics\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "import logging\n",
    "import time\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        return get_ipython().__class__.__name__ == 'ZMQInteractiveShell'\n",
    "    except ModuleNotFoundError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook():\n",
    "    n_procs = multiprocessing.cpu_count()//2\n",
    "    random_tasks = 2\n",
    "else:\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n-processes\", type=int, required=True)\n",
    "    parser.add_argument(\"--random-tasks\", type=int, default=0)\n",
    "    args = parser.parse_args()\n",
    "    n_procs = args.n_processes\n",
    "    random_tasks = args.random_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(name)s %(asctime)s - %(message)s')\n",
    "logging.captureWarnings(True)\n",
    "logging.getLogger(\"openml\").setLevel(logging.ERROR)\n",
    "\n",
    "class NoRequestFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return not \"request for the URL\" in record.getMessage()\n",
    "\n",
    "logging.getLogger(\"root\").addFilter(NoRequestFilter())\n",
    "logging.getLogger().addFilter(NoRequestFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare metrics over datasets\n",
    "\n",
    "In this notebook we collect calibration estimates for several models and datasets. The estimates are computed with the Expected Calibration Error (ECE), Balanced-ECE, and Posterior Expected Accuracy-based Calibration Error (PEACE). A list of manually selected datasets is available, but in final experiments we used randomly selected datasets from the OpenML repository.\n",
    "\n",
    "Tested models:\n",
    "- Random Forest\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Gaussian Naive Bayes\n",
    "- MLP\n",
    "- AdaBoost\n",
    "- Decision Tree\n",
    "\n",
    "Selected datasets (from Open ML):\n",
    "- [eeg-eye-state](https://www.openml.org/d/1471), task [9983](https://www.openml.org/t/9983)\n",
    "- [Phoneme](https://www.openml.org/d/1489), task [9952](https://www.openml.org/t/9952)\n",
    "- [mozilla4](https://www.openml.org/d/1046), task [3899](https://www.openml.org/t/3899)\n",
    "- [electricity](https://www.openml.org/d/151), task [219](https://www.openml.org/t/219)\n",
    "- [Magic Telescope](https://www.openml.org/d/1120), task [3954](https://www.openml.org/t/3954)\n",
    "- [artificial characters](https://www.openml.org/d/1459), task [14964](https://www.openml.org/t/14964)\n",
    "- [pendigits](https://www.openml.org/d/32), task [32](https://www.openml.org/t/32)\n",
    "- [letter](https://www.openml.org/d/6), task [6](https://www.openml.org/t/6)\n",
    "- [JapaneseVowels](https://www.openml.org/d/375), task [3510](https://www.openml.org/t/3510)\n",
    "- [glass](https://www.openml.org/d/41), task [40](https://www.openml.org/t/40)\n",
    "- [micro-mass](https://www.openml.org/d/1515), task [9950](https://www.openml.org/t/9950)\n",
    "- [vehicle](https://www.openml.org/d/54), task [53](https://www.openml.org/t/53)\n",
    "- [synthetic-control](https://www.openml.org/d/377), task [3512](https://www.openml.org/t/3512)\n",
    "- [mfeat-factors](https://www.openml.org/d/12), task [12](https://www.openml.org/t/12)\n",
    "- [OVA_breast](https://www.openml.org/d/1128), task [3962](https://www.openml.org/d/1128)\n",
    "- [sonar](https://www.openml.org/d/40), task [39](https://www.openml.org/t/39)\n",
    "- [visualizing_livestock](https://www.openml.org/d/685), task [3577](https://www.openml.org/t/3577)\n",
    "- [spectrometer](https://www.openml.org/d/313), task [145682](https://www.openml.org/t/145682)\n",
    "- [disclosure_z](https://www.openml.org/d/931), task [3794](https://www.openml.org/t/3794)\n",
    "- [mfeat-pixel](https://www.openml.org/d/40979), task [146824](https://www.openml.org/t/146824)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "First of all, we need to be able to load and preprocess multiple datasets appropriately to prepare it for classification. \n",
    "To this end we implement a function that will load a dataset and classification task from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_random_task(selected_tasks):\n",
    "    tries = 0\n",
    "    while True:\n",
    "        df = openml.tasks.list_tasks(task_type_id=1, offset=tries*1000, output_format=\"dataframe\", size=1000, status=\"active\", number_missing_values=0)\n",
    "        tries += 1\n",
    "        if \"NumberOfInstances\" in df:\n",
    "            df = df[(df[\"NumberOfInstances\"] > 2000) & (df[\"NumberOfInstances\"] < 10000)]\n",
    "            df = df[~df[\"tid\"].isin(list(selected_tasks))]\n",
    "            if len(df) > 0:\n",
    "                task = df.sample(n=1).iloc[0][\"tid\"]\n",
    "                selected_tasks.append(task)\n",
    "                return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_TASKS = [  3919,   4230,   3601,     30,   3891,   4190,   1792,     43,\n",
    "         1908,   3786,   2939,   3488,   3843,   3603,    266,   4193,\n",
    "         3528,   3842,   3525,   3777,   1923,   1809,    275,   3638,\n",
    "         3684,   1800,   1916,   1794,   2074,     36,   3672,   2120,\n",
    "          288,   3594,   3712,   3618,   3586,   4229,   3917,   1925,\n",
    "         3816,   1767,   3668,   1883,   3698,   3907,   4233,   3485,\n",
    "         3531,   3627,   1822,   3524,   3821,   3681,   3892,   3950,\n",
    "          233,    260,   3894,   4236,   3884,   3735,   4215,   2121,\n",
    "          258,   3481,   3839,   3510,     58,   1910,   3591,     28,\n",
    "          273,     45,      3,   1938,   1807,   4186, 126022,  14969,\n",
    "       145945,  75100,   4308, 145855,   9898,   4612,  75203,  75216,\n",
    "       145891, 145985,   9952,   9921,  75169, 145903,  75242,  75124,\n",
    "        75138,  10091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_TASKS = [4240, 4245, 1780,  248, 1896, 3891, 3934, 3611, 3837, 3763, 1784,\n",
    "       2938,   12,  242, 3918, 3497, 3524, 3614, 3523,  246, 1923, 3925,\n",
    "        277,   31, 3603, 3653, 3894,   52, 3058,  250, 4196, 3994,    3,\n",
    "       3584,  252, 4200, 3527, 1787, 3778, 3993, 3841, 3959, 3996, 3846,\n",
    "       4190, 3933,  275, 1927, 4234, 4238, 3997, 3520, 1892, 4241, 1811,\n",
    "        266, 3521, 1806, 3635, 1916, 3730, 3806, 3710, 3892, 3657, 3775,\n",
    "        270, 3491, 3716, 3753, 3676, 3788, 3538, 4235, 3585, 3731, 3702,\n",
    "       3917, 3821,   45, 3689, 3598,  233, 3995, 2104, 3519, 3735, 1903,\n",
    "       3777, 4243, 3828, 3733, 3056, 3583, 4225, 3695, 4198, 3617,  261]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_tasks > 0:\n",
    "    TASKS = random_tasks\n",
    "else:\n",
    "    TASKS = LARGE_TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openml_task(task_id=None, selected_tasks=[]):\n",
    "    \n",
    "    while True:\n",
    "        if task_id is None:\n",
    "            curr_id = find_random_task(selected_tasks)\n",
    "        else:\n",
    "            curr_id = task_id\n",
    "\n",
    "        try: \n",
    "            task = openml.tasks.get_task(curr_id)\n",
    "            X, y = task.get_X_and_y(\"array\")\n",
    "            X, y = sklearn.utils.indexable(X, y)\n",
    "            \n",
    "            target_type = sklearn.utils.multiclass.type_of_target(y)\n",
    "            if target_type not in [\"binary\", \"multiclass\"]:\n",
    "                continue\n",
    "\n",
    "            if hasattr(X, \"toarray\"):\n",
    "                X = X.toarray()\n",
    "            if hasattr(y, \"toarray\"):\n",
    "                y = y.toarray()\n",
    "\n",
    "            X = sklearn.preprocessing.OrdinalEncoder().fit_transform(X)\n",
    "\n",
    "            n_repeats, n_folds, n_samples = task.get_split_dimensions()\n",
    "            \n",
    "            if n_folds > 10:\n",
    "                continue\n",
    "\n",
    "            folds = numpy.full((len(X)), dtype=int, fill_value=-1)\n",
    "            for fold_idx in range(n_folds):\n",
    "                _, test_indices = task.get_train_test_split_indices(\n",
    "                    repeat=0,\n",
    "                    fold=fold_idx,\n",
    "                    sample=0,\n",
    "                )\n",
    "\n",
    "                folds[test_indices] = fold_idx\n",
    "\n",
    "            splitter = sklearn.model_selection.PredefinedSplit(folds)\n",
    "\n",
    "            return X, y, splitter, curr_id\n",
    "        except Exception as e:\n",
    "            if task_id is not None:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noscript\n",
    "from tqdm.notebook import trange\n",
    "iter_tasks = []\n",
    "tmp_tasks = []\n",
    "for i in trange(100):\n",
    "    _, _, _, task = load_openml_task(selected_tasks=tmp_tasks)\n",
    "    iter_tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noscript\n",
    "X, y, splitter, task_id = load_openml_task(9952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noscript\n",
    "X, y, splitter, task_id = load_openml_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting calibration metrics\n",
    "\n",
    "For each dataset we fit and evaluate multiple models. We record calibration metrics (ECE, ECE-balanced, PEACE), and some performance metrics (balanced accuracy, F1-score, accuracy). To this end we implement a function that runs this procedure for one model on one OpenML task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noscript\n",
    "MODELS = {\n",
    "    \"rf\": sklearn.ensemble.RandomForestClassifier(),\n",
    "    \"svm\": sklearn.svm.SVC(probability=True),\n",
    "    \"logreg\": sklearn.linear_model.LogisticRegression(max_iter=1000),\n",
    "    \"nb\": sklearn.naive_bayes.GaussianNB(),\n",
    "    \"mlp\": sklearn.neural_network.MLPClassifier(max_iter=1000),\n",
    "    \"adaboost\": sklearn.ensemble.AdaBoostClassifier(n_estimators=500),\n",
    "    \"dectree\": sklearn.tree.DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"adaboost\": sklearn.ensemble.AdaBoostClassifier(n_estimators=500),\n",
    "    \"dectree\": sklearn.tree.DecisionTreeClassifier(),\n",
    "    \"svm\": sklearn.svm.LinearSVC(),\n",
    "    \"xgb\": xgboost.XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noscript\n",
    "MODELS = {\n",
    "    \"xgb\": xgboost.XGBClassifier(),\n",
    "    \"svm\": sklearn.svm.LinearSVC()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noscript\n",
    "MODELS = {\n",
    "    \"svm\": sklearn.ensemble.BaggingClassifier(sklearn.svm.SVC(probability=True)),\n",
    "    \"logreg\": sklearn.ensemble.BaggingClassifier(sklearn.linear_model.LogisticRegression(max_iter=1000)),\n",
    "    \"nb\": sklearn.ensemble.BaggingClassifier(sklearn.naive_bayes.GaussianNB()),\n",
    "    \"mlp\": sklearn.ensemble.BaggingClassifier(sklearn.neural_network.MLPClassifier(max_iter=1000)),\n",
    "    \"dectree\": sklearn.ensemble.BaggingClassifier(sklearn.tree.DecisionTreeClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noscript\n",
    "MODELS = {\n",
    "    \"dummy\": sklearn.dummy.DummyClassifier(strategy=\"prior\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(model_id, Xt, yt, Xv, yv):\n",
    "    # get and fit fresh model\n",
    "    model = sklearn.base.clone(MODELS[model_id])\n",
    "    model.fit(Xt, yt)\n",
    "\n",
    "    # predict on test\n",
    "    y_probs = model.predict_proba(Xv)\n",
    "    y_preds = model.predict(Xv)\n",
    "    \n",
    "    return y_probs, y_preds, yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_metrics_for_model_and_task(model_id, task_id, pool, counter, start_at, selected_tasks):\n",
    "    X, y, splitter, task_id = load_openml_task(task_id, selected_tasks=selected_tasks) # repeated runs will use cached data\n",
    "    \n",
    "    row = {\n",
    "        \"model_id\": model_id,\n",
    "        \"task_id\": task_id,\n",
    "    }\n",
    "\n",
    "    promises = []\n",
    "    for i, (train_idx, test_idx) in enumerate(splitter.split()):\n",
    "        counter += 1\n",
    "        if counter < start_at:\n",
    "            continue\n",
    "\n",
    "        # split data\n",
    "        Xt, yt = X[train_idx], y[train_idx]\n",
    "        Xv, yv = X[test_idx], y[test_idx]\n",
    "\n",
    "        promise = pool.apply_async(\n",
    "            fit_and_predict,\n",
    "            (model_id, Xt, yt, Xv, yv)\n",
    "        )\n",
    "        promises.append(promise)\n",
    "\n",
    "    logging.info(f\"Promises for single cv: {len(promises)}\")\n",
    "    return row, promises, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection over all datasets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: root 2020-10-07 11:34:37,250 - Output to metrics_1602063277.dat\n",
      "INFO: root 2020-10-07 11:34:58,369 - Tasks: [3919, 4230]\n",
      "INFO: root 2020-10-07 11:34:58,369 - Start submitting promises for adaboost\n",
      "INFO: root 2020-10-07 11:34:58,412 - Promises for single cv: 10\n",
      "INFO: root 2020-10-07 11:34:58,413 - 1 tasks submitted to pool (adaboost)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d2f3cc26dd95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpromises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_tasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cv_metrics_for_model_and_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_at\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mpromises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{len(promises)} tasks submitted to pool ({model_id})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-538a4fa8bd8e>\u001b[0m in \u001b[0;36mget_cv_metrics_for_model_and_task\u001b[0;34m(model_id, task_id, pool, counter, start_at, selected_tasks)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_cv_metrics_for_model_and_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_at\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_openml_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_tasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# repeated runs will use cached data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     row = {\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"model_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-a498c341f733>\u001b[0m in \u001b[0;36mload_openml_task\u001b[0;34m(task_id, selected_tasks)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mn_repeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_split_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \"\"\"\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mXi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mcats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mcats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         return _encode_numpy(values, uniques, encode,\n\u001b[0m\u001b[1;32m    122\u001b[0m                              check_unknown=check_unknown)\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode_numpy\u001b[0;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# unique sorts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m def unique(ar, return_index=False, return_inverse=False,\n\u001b[1;32m    153\u001b[0m            return_counts=False, axis=None):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool(processes=n_procs) as pool:\n",
    "\n",
    "    start_at = 0\n",
    "\n",
    "    output_file = f\"metrics_{int(time.time())}.dat\"\n",
    "    logging.info(f\"Output to {output_file}\")\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    if type(TASKS) is int:\n",
    "        iter_tasks = []\n",
    "        tmp_tasks = []\n",
    "        for i in range(TASKS):\n",
    "            _, _, _, task = load_openml_task(selected_tasks=tmp_tasks)\n",
    "            iter_tasks.append(task)\n",
    "    else:\n",
    "        iter_tasks = TASKS\n",
    "        \n",
    "    logging.info(f\"Tasks: {iter_tasks}\")\n",
    "    \n",
    "    data = []\n",
    "    for model_id in MODELS.keys():\n",
    "        \n",
    "        logging.info(f\"Start submitting promises for {model_id}\")\n",
    "        \n",
    "        promises = []\n",
    "        for task_id in iter_tasks:\n",
    "            row, tmp, counter = get_cv_metrics_for_model_and_task(model_id, task_id, pool, counter, start_at, [])\n",
    "            promises.append((row, tmp))\n",
    "            logging.info(f\"{len(promises)} tasks submitted to pool ({model_id})\")\n",
    "        logging.info(f\"All {len(promises)} tasks submitted to pool ({model_id})\")\n",
    "\n",
    "        for i, (row, promise) in enumerate(promises):\n",
    "            try:\n",
    "                y_probs, y_preds, y_test = [], [], []\n",
    "                for x in promise:\n",
    "                    x = x.get()\n",
    "                    y_probs.extend(x[0])\n",
    "                    y_preds.extend(x[1])\n",
    "                    y_test.extend(x[2])\n",
    "\n",
    "                # stack fold results and compute metrics\n",
    "                y_probs = numpy.array(y_probs)\n",
    "                logging.debug(y_probs.shape)\n",
    "                y_probs_max = y_probs.max(axis=1)\n",
    "                y_preds = numpy.array(y_preds)\n",
    "                y_test = numpy.array(y_test)\n",
    "\n",
    "                bins = 15\n",
    "                row.update({\n",
    "                    \"accuracy\": sklearn.metrics.accuracy_score(y_test, y_preds),\n",
    "                    \"balanced_accuracy\": sklearn.metrics.balanced_accuracy_score(y_test, y_preds),\n",
    "                    \"f1\": sklearn.metrics.f1_score(y_test, y_preds, average=\"weighted\"),\n",
    "                    'ece': metrics.ece(y_probs_max, y_preds, y_test, bins=bins),\n",
    "                    'ece_balanced': metrics.ece(y_probs_max, y_preds, y_test, balanced=True, bins=bins),\n",
    "                    'peace': metrics.peace(y_probs_max, y_preds, y_test, bins=bins),\n",
    "                    'class_wise_ece': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.ece, bins=bins),\n",
    "                    'class_wise_peace': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.peace, bins=bins)\n",
    "                })\n",
    "\n",
    "                # update data and dump intermediate dataframe\n",
    "                data.append(row)\n",
    "                df = pandas.DataFrame(data)\n",
    "                dump(df, output_file)\n",
    "\n",
    "                logging.info(f\"Finished tasks: {i+1}/{len(promises)} ({(i+1)/len(promises)*100:.2f}%)\")\n",
    "            except Exception:\n",
    "                logging.exception(\"Exception when collecting results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_notebook():\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = load(\"/home/maximl/Data/Experiment_data/results/riverrel/datasets/random_openml/metrics_1601995877.dat\")\n",
    "tmp_df[\"model_id\"] = \"bagged_\" + tmp_df[\"model_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.concat([\n",
    "#     load(\"/home/maximl/Data/Experiment_data/results/riverrel/datasets/random_openml/metrics_1601494658.dat\"),\n",
    "    load(\"/home/maximl/Data/Experiment_data/results/riverrel/datasets/random_openml/metrics_1602009416.dat\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"model_id\", \"task_id\", \"Accuracy\", \"Balanced Accuracy\", \"F1\", \"ECE\", \"Balanced ECE\", \"PEACE\", \"cw-ECE\", \"cw-PEACE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longform(df, cols=None, subject_cols=None):\n",
    "    dfs = []\n",
    "    \n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    \n",
    "    for col in cols:\n",
    "        tmp_df = pandas.DataFrame(dict(        \n",
    "            value=df[col], \n",
    "            metric=col,   \n",
    "        ))\n",
    "        for col2 in set(df.columns) - set(cols):\n",
    "            tmp_df[col2] = df[col2]\n",
    "            \n",
    "        if subject_cols is not None:\n",
    "            tmp_df[\"subject\"] = df[subject_cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "            \n",
    "        dfs.append(tmp_df)\n",
    "        \n",
    "    return pandas.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = get_longform(df, df.columns[2:], [\"model_id\", \"task_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set_theme(\"paper\", \"whitegrid\", font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "seaborn.boxplot(\n",
    "    data=long_df[long_df[\"metric\"].isin([\"Accuracy\", \"Balanced Accuracy\", \"F1\"])], \n",
    "    x=\"model_id\", y=\"value\", hue=\"metric\", saturation=.7, ax=ax\n",
    ")\n",
    "seaborn.stripplot(\n",
    "    data=long_df[long_df[\"metric\"].isin([\"Accuracy\", \"Balanced Accuracy\", \"F1\"])], \n",
    "    x=\"model_id\", y=\"value\", hue=\"metric\", dodge=True, color=\".25\", s=3, alpha=.8, ax=ax\n",
    ")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Metric value\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xticklabels(\n",
    "#     [\"AdaBoost\", \"Decision Tree\", \"Logistic Regression\", \"Multi-layer Perceptron\", \"Gaussian Naive Bayes\", \"Random Forest\", \"SVM\"],\n",
    "    rotation=-30, ha=\"left\")\n",
    "\n",
    "plt.legend(handles[:3], labels[:3], bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right',\n",
    "           ncol=3, borderaxespad=0.)\n",
    "plt.savefig(\"performance.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"ECE\", \"Balanced ECE\", \"PEACE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.displot(data=long_df[long_df[\"metric\"].isin(cols)], x=\"value\", hue=\"metric\", rug=True, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = seaborn.violinplot(data=long_df[long_df[\"metric\"].isin(cols)], y=\"value\", x=\"metric\")\n",
    "seaborn.stripplot(data=long_df[long_df[\"metric\"].isin(cols)], x=\"metric\", y=\"value\", color=\".25\", s=2, alpha=.8)\n",
    "g.set_xlabel(\"\")\n",
    "g.set_ylabel(\"Metric value\")\n",
    "# plt.savefig(\"metrics.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[\"PEACE\"] - df[\"ECE\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friedman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_stars(p):\n",
    "    if p < 0.001:\n",
    "        return \"*\"*3\n",
    "    elif p < 0.01:\n",
    "        return \"*\"*2\n",
    "    elif p < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = seaborn.FacetGrid(data=df, col=\"model_id\", col_wrap=4)\n",
    "table_data = [[\"PEACE - ECE\"], [\"PEACE - Balanced ECE\"], [\"Balanced ECE - ECE\"]]\n",
    "for ax, (idx, model_df) in zip(grid.axes, df.groupby(\"model_id\")):    \n",
    "    \n",
    "    diff_df = pandas.concat([\n",
    "        pandas.DataFrame({\"value\": model_df[\"PEACE\"] - model_df[\"ECE\"], \"diff\": \"PEACE - ECE\"}),\n",
    "        pandas.DataFrame({\"value\": model_df[\"PEACE\"] - model_df[\"Balanced ECE\"], \"diff\": \"PEACE - Balanced ECE\"}),\n",
    "        pandas.DataFrame({\"value\": model_df[\"Balanced ECE\"] - model_df[\"ECE\"], \"diff\": \"Balanced ECE - ECE\"})\n",
    "    ])\n",
    "    \n",
    "    data = model_df.loc[:, cols + [\"task_id\"]]\n",
    "    test = scipy.stats.friedmanchisquare(data.iloc[:, 0], data.iloc[:, 1], data.iloc[:, 2])\n",
    "    if test.pvalue < 0.05:\n",
    "        a = sp.posthoc_conover_friedman(data.iloc[:, :3], p_adjust=\"bonferroni\")\n",
    "        colors = [\n",
    "            \"red\" if a[\"PEACE\"][\"ECE\"] < 0.05 else \"grey\",\n",
    "            \"red\" if a[\"PEACE\"][\"Balanced ECE\"] < 0.05 else \"grey\",\n",
    "            \"red\" if a[\"Balanced ECE\"][\"ECE\"] < 0.05 else \"grey\"\n",
    "        ]\n",
    "    else:\n",
    "        colors = [\"grey\"]*3\n",
    "\n",
    "    tmp_df = diff_df.groupby(\"diff\").aggregate(\"mean\")\n",
    "    for i, (col1, col2) in enumerate([[\"PEACE\", \"ECE\"], [\"PEACE\", \"Balanced ECE\"], [\"Balanced ECE\", \"ECE\"]]):\n",
    "        table_data[i].append(\"$%+.3f$ %s\" % (tmp_df.loc[f\"{col1} - {col2}\"], map_stars(a[col1][col2])))\n",
    "    \n",
    "    ax.set_title(idx)\n",
    "    seaborn.boxplot(data=diff_df, y=\"diff\", x=\"value\", orient=\"h\", ax=ax, palette=seaborn.color_palette(colors))\n",
    "    ax.set_xlim(-0.05, 0.05)\n",
    "\n",
    "grid.set_axis_labels(\"Difference\", \"Comparison\")\n",
    "plt.savefig(\"pairwise_comparisons.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grid = seaborn.FacetGrid(data=df1, col=\"model_id\", col_wrap=4)\n",
    "table_data = [[\"PEACE - ECE\"], [\"PEACE - Balanced ECE\"], [\"Balanced ECE - ECE\"]]\n",
    "for ax, (idx, model_df) in zip(grid.axes, df1.groupby(\"model_id\")):    \n",
    "    \n",
    "    diff_df = pandas.concat([\n",
    "        pandas.DataFrame({\"value\": model_df[\"PEACE\"] - model_df[\"ECE\"], \"diff\": \"PEACE - ECE\"}),\n",
    "        pandas.DataFrame({\"value\": model_df[\"PEACE\"] - model_df[\"Balanced ECE\"], \"diff\": \"PEACE - Balanced ECE\"}),\n",
    "        pandas.DataFrame({\"value\": model_df[\"Balanced ECE\"] - model_df[\"ECE\"], \"diff\": \"Balanced ECE - ECE\"})\n",
    "    ])\n",
    "    \n",
    "    data = model_df.loc[:, cols + [\"task_id\"]]\n",
    "    test = scipy.stats.friedmanchisquare(data.iloc[:, 0], data.iloc[:, 1], data.iloc[:, 2])\n",
    "    if test.pvalue < 0.05:\n",
    "        a = sp.posthoc_conover_friedman(data.iloc[:, :3], p_adjust=\"bonferroni\")\n",
    "        colors = [\n",
    "            \"red\" if a[\"PEACE\"][\"ECE\"] < 0.05 else \"grey\",\n",
    "            \"red\" if a[\"PEACE\"][\"Balanced ECE\"] < 0.05 else \"grey\",\n",
    "            \"red\" if a[\"Balanced ECE\"][\"ECE\"] < 0.05 else \"grey\"\n",
    "        ]\n",
    "    else:\n",
    "        colors = [\"grey\"]*3\n",
    "\n",
    "    tmp_df = diff_df.groupby(\"diff\").aggregate(\"mean\")\n",
    "    for i, (col1, col2) in enumerate([[\"PEACE\", \"ECE\"], [\"PEACE\", \"Balanced ECE\"], [\"Balanced ECE\", \"ECE\"]]):\n",
    "        table_data[i].append(\"$%+.3f$ %s\" % (tmp_df.loc[f\"{col1} - {col2}\"], map_stars(a[col1][col2])))\n",
    "    \n",
    "    ax.set_title(idx)\n",
    "    seaborn.boxplot(data=diff_df, y=\"diff\", x=\"value\", orient=\"h\", ax=ax, palette=seaborn.color_palette(colors))\n",
    "    ax.set_xlim(-0.05, 0.05)\n",
    "\n",
    "grid.set_axis_labels(\"Difference\", \"Comparison\")\n",
    "plt.savefig(\"pairwise_comparisons.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"AdaBoost\", \"DecTree\", \"LogReg\", \"MLP\", \"GNB\", \"RF\", \"SVM\"]\n",
    "print(tabulate.tabulate(table_data, headers=headers, tablefmt=\"latex\").replace(\"\\\\$\", \"$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin(cols)], x=\"metric\", y=\"value\", col=\"model_id\", kind=\"violin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Randomly selected datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_meta(task_id):\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    d = task.get_dataset()\n",
    "    return {\n",
    "        \"Task ID\": task_id,\n",
    "        \"# classes\": d.qualities[\"NumberOfClasses\"],\n",
    "        \"# features\":  d.qualities[\"NumberOfFeatures\"],\n",
    "        \"# instances\": d.qualities[\"NumberOfInstances\"],\n",
    "        \"Dataset ID\": d.dataset_id,\n",
    "        \"Dataset name\": d.name,\n",
    "        \"Dataset URL\": d.openml_url,\n",
    "        \"Dataset version\": d.version,\n",
    "        \"Dataset creator\": \", \".join(d.creator) if type(d.creator) is list else d.creator\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "    tasks = pool.map(get_task_meta, df[\"task_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = pandas.DataFrame(tasks)\n",
    "tasks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task_table.tex\", \"w\") as fp:\n",
    "    fp.write(tabulate.tabulate(\n",
    "        tasks[[\"Task ID\", \"Dataset ID\", \"Dataset name\", \"Dataset version\", \"# classes\", \"# features\", \"# instances\"]].values, \n",
    "        headers=[\"Task ID\", \"Dataset ID\", \"Dataset name\", \"Dataset version\", \"# classes\", \"# features\", \"# instances\"],\n",
    "        tablefmt=\"latex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tasks.merge(df, right_on=\"task_id\", left_on=\"Task ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks[\"# instances\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[\"PEACE-ECE\"] = table[\"PEACE\"] - table[\"ECE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[\"PEACE-balECE\"] = table[\"PEACE\"] - table[\"Balanced ECE\"]\n",
    "table[\"PEACE>=balECE\"] = table[\"PEACE\"] >= table[\"Balanced ECE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = table[\"model_id\"] == \"mlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 4), constrained_layout=True)\n",
    "seaborn.regplot(data=table, x=\"# instances\", y=\"PEACE-balECE\", ax=ax[0])\n",
    "seaborn.regplot(data=table, x=\"# features\", y=\"PEACE-balECE\", ax=ax[1])\n",
    "seaborn.regplot(data=table, x=\"# classes\", y=\"PEACE-balECE\", ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pandas.concat([pandas.get_dummies(table[\"Task ID\"]), pandas.get_dummies(table[\"model_id\"]), table[[\"# instances\", \"# features\", \"# classes\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.RLM(table[\"PEACE-balECE\"], sm.add_constant(X))\n",
    "res = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = grouped_df.iloc[(grouped_df[\"PEACE\"] - grouped_df[\"ECE\"]).sort_values().index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(model):\n",
    "    X, y, splitter, task_id = load_openml_task(row[\"task_id\"])\n",
    "\n",
    "    y_probs = [[]]*splitter.get_n_splits()\n",
    "    y_preds = [[]]*splitter.get_n_splits()\n",
    "    y_test = [[]]*splitter.get_n_splits()\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(splitter.split()):\n",
    "        # split data\n",
    "        Xt, yt = X[train_idx], y[train_idx]\n",
    "        Xv, yv = X[test_idx], y[test_idx]\n",
    "\n",
    "        # train adaboost\n",
    "        model_instance = model()\n",
    "        model_instance.fit(Xt, yt)\n",
    "\n",
    "        y_probs[i] = model_instance.predict_proba(Xv)\n",
    "        y_preds[i] = model_instance.predict(Xv)\n",
    "        y_test[i] = yv\n",
    "        \n",
    "    return y_probs, y_preds, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_data = func(sklearn.ensemble.AdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_data = func(partial(sklearn.linear_model.LogisticRegression, max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_data = func(partial(sklearn.svm.SVC, max_iter=1000, probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_data = func(partial(sklearn.neural_network.MLPClassifier, max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = func(sklearn.ensemble.RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharex=True, sharey=True, figsize=(20, 4))\n",
    "\n",
    "for ax, data in zip(axes, [ada_data, logreg_data, svm_data, mlp_data, rf_data]):\n",
    "    y_probs = numpy.hstack([p.max(axis=1) for p in data[0]])\n",
    "    y_preds = numpy.hstack(data[1])\n",
    "    y_test = numpy.hstack(data[2])\n",
    "    ax.set_title(f\"ECE: {metrics.ece(y_probs, y_preds, y_test, bins=15):.3f}, PEACE: {metrics.peace(y_probs, y_preds, y_test, bins=15):.3f}\")\n",
    "\n",
    "    seaborn.histplot(y_probs, ax=ax, bins=numpy.histogram_bin_edges(y_probs, bins=15, range=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.river_reliability_diagram(mlp_data[0][i].max(axis=1), mlp_data[1][i], mlp_data[2][i], bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.river_reliability_diagram(logreg_data[0][i].max(axis=1), logreg_data[1][i], logreg_data[2][i], bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.confidence_reliability_diagram(ada_data[0][i].max(axis=1), ada_data[1][i], ada_data[2][i], bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.confidence_reliability_diagram(logreg_data[0][i].max(axis=1), logreg_data[1][i], logreg_data[2][i], bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
