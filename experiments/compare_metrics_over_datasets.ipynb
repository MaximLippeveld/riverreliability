{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook compare_metrics_over_datasets.ipynb to python\n",
      "[NbConvertApp] Writing 7408 bytes to compare_metrics_over_datasets.py\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "!jupyter nbconvert --RegexRemovePreprocessor.patterns=\"['# ?noscript']\" --TemplateExporter.exclude_markdown=True --to python compare_metrics_over_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import openml\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.base\n",
    "import sklearn.utils\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.neural_network\n",
    "\n",
    "from ridgereliability import plots, metrics\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "import logging\n",
    "import time\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_ipython().__class__.__name__ != 'ZMQInteractiveShell':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n-processes\", type=int, required=True)\n",
    "    n_procs = parser.parse_args().n_processes\n",
    "else:\n",
    "    n_procs = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare metrics over datasets\n",
    "\n",
    "In this notebook we collect calibration estimates for several models and datasets. The estimates are computed with the Expected Calibration Error (ECE), Balanced-ECE, and Posterior Expected Accuracy-based Calibration Error (PEACE).\n",
    "\n",
    "Tested models:\n",
    "- Random Forest\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Gaussian Naive Bayes\n",
    "- MLP\n",
    "\n",
    "Tested datasets (from Open ML):\n",
    "- [eeg-eye-state](https://www.openml.org/d/1471), task [9983](https://www.openml.org/t/9983)\n",
    "- [Phoneme](https://www.openml.org/d/1489), task [9952](https://www.openml.org/t/9952)\n",
    "- [mozilla4](https://www.openml.org/d/1046), task [3899](https://www.openml.org/t/3899)\n",
    "- [electricity](https://www.openml.org/d/151), task [219](https://www.openml.org/t/219)\n",
    "- [Magic Telescope](https://www.openml.org/d/1120), task [3954](https://www.openml.org/t/3954)\n",
    "- [artificial characters](https://www.openml.org/d/1459), task [14964](https://www.openml.org/t/14964)\n",
    "- [pendigits](https://www.openml.org/d/32), task [32](https://www.openml.org/t/32)\n",
    "- [letter](https://www.openml.org/d/6), task [6](https://www.openml.org/t/6)\n",
    "- [JapaneseVowels](https://www.openml.org/d/375), task [3510](https://www.openml.org/t/3510)\n",
    "- [glass](https://www.openml.org/d/41), task [40](https://www.openml.org/t/40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "First of all, we need to be able to load and preprocess multiple datasets appropriately to prepare it for classification. \n",
    "To this end we implement a function that will load a dataset and classification task from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [9983, 9952, 3899, 219, 3954, 14964, 32, 6, 3510, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openml_task(task_id):\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    X, y = task.get_X_and_y(\"dataframe\")\n",
    "    n_repeats, n_folds, n_samples = task.get_split_dimensions()\n",
    "\n",
    "    folds = numpy.empty((len(X)), dtype=int)\n",
    "    for fold_idx in range(n_folds):\n",
    "        _, test_indices = task.get_train_test_split_indices(\n",
    "            repeat=0,\n",
    "            fold=fold_idx,\n",
    "            sample=0,\n",
    "        )\n",
    "        \n",
    "        folds[test_indices] = fold_idx\n",
    "        \n",
    "    splitter = sklearn.model_selection.PredefinedSplit(folds)\n",
    "                \n",
    "    return X, y, splitter        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: 2020-09-24 16:01:38,838 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:38,843 - Data pickle file already exists and is up to date.\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "X, y, splitter = load_openml_task(9952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(541,)\n",
      "(541,)\n",
      "(541,)\n",
      "(541,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "for train_idx, test_idx in splitter.split():\n",
    "    print(test_idx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting calibration metrics\n",
    "\n",
    "For each dataset we fit and evaluate multiple models. We record calibration metrics (ECE, ECE-balanced, PEACE), and some performance metrics (balanced accuracy, F1-score, accuracy). To this end we implement a function that runs this procedure for one model on one OpenML task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"rf\": sklearn.ensemble.RandomForestClassifier(),\n",
    "    \"svm\": sklearn.svm.SVC(probability=True),\n",
    "    \"logreg\": sklearn.linear_model.LogisticRegression(max_iter=500),\n",
    "    \"nb\": sklearn.naive_bayes.GaussianNB(),\n",
    "    \"mlp\": sklearn.neural_network.MLPClassifier(max_iter=500)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_metrics_for_model(row, Xt, yt, Xv, yv):\n",
    "\n",
    "    # get and fit fresh model\n",
    "    model = sklearn.base.clone(MODELS[row[\"model_id\"]])\n",
    "    model.fit(Xt, yt)\n",
    "\n",
    "    # compute metrics on test data\n",
    "    y_probs = model.predict_proba(Xv)\n",
    "    y_probs_max = y_probs.max(axis=1)\n",
    "    y_preds = model.predict(Xv)\n",
    "    y_test = yv\n",
    "\n",
    "    bins = 15\n",
    "    row.update({\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(y_test, y_preds),\n",
    "        \"balanced_accuracy\": sklearn.metrics.balanced_accuracy_score(y_test, y_preds),\n",
    "        \"f1\": sklearn.metrics.f1_score(y_test, y_preds, average=\"weighted\"),\n",
    "        'ece': metrics.ece(y_probs_max, y_preds, y_test, bins=bins),\n",
    "        'ece_balanced': metrics.ece(y_probs_max, y_preds, y_test, balanced=True, bins=bins),\n",
    "        'peace': metrics.peace(y_probs_max, y_preds, y_test, bins=bins),\n",
    "        'class_wise_ece': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.ece, bins=bins),\n",
    "        'class_wise_peace': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.peace, bins=bins)\n",
    "    })\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_metrics_for_model_and_task(model_id, task_id, pool, n_repeats, counter, start_at):\n",
    "    \n",
    "    X, y, splitter = load_openml_task(task_id) # repeated runs will use cached data\n",
    "    \n",
    "    promises = []\n",
    "    for i, (train_idx, test_idx) in enumerate(splitter.split()):\n",
    "        for j in range(n_repeats):\n",
    "            counter += 1\n",
    "            if counter < start_at:\n",
    "                continue\n",
    "            \n",
    "            numpy.random.seed(j)\n",
    "            \n",
    "            row = {\n",
    "                \"fold\": i,\n",
    "                \"repeat\": j,\n",
    "                \"model_id\": model_id,\n",
    "                \"task_id\": task_id,\n",
    "            }\n",
    "\n",
    "            # split data\n",
    "            Xt, yt = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            Xv, yv = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "            promise = pool.apply_async(\n",
    "                get_fold_metrics_for_model,\n",
    "                (row, Xt, yt, Xv, yv)\n",
    "            )\n",
    "            promises.append(promise)\n",
    "        \n",
    "    return promises, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection over all datasets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2020-09-24 16:01:38,937 - Output to metrics_1600956098.dat\n",
      "DEBUG: 2020-09-24 16:01:38,943 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:38,949 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:38,958 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:38,962 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:38,968 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:38,973 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:38,982 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:38,993 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,011 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,016 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 16:01:39,017 - Going to remove the following attributes: ['ID']\n",
      "DEBUG: 2020-09-24 16:01:39,027 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,031 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,038 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,043 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,052 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,058 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,068 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,073 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,081 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,084 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,090 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,095 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,103 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,107 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,113 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,118 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,126 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,136 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,154 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,160 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 16:01:39,161 - Going to remove the following attributes: ['ID']\n",
      "DEBUG: 2020-09-24 16:01:39,170 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,174 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,182 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,187 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,195 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,201 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,212 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,217 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,225 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,229 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,235 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,240 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,249 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,252 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,258 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,264 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,272 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,283 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,301 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,307 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 16:01:39,308 - Going to remove the following attributes: ['ID']\n",
      "DEBUG: 2020-09-24 16:01:39,317 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,321 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,329 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,334 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,343 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,348 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,359 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,364 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,371 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,375 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,380 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,385 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,394 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,397 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,403 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,407 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,416 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,425 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,442 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,447 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 16:01:39,448 - Going to remove the following attributes: ['ID']\n",
      "DEBUG: 2020-09-24 16:01:39,458 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,462 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,469 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,473 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,481 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,486 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,497 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,502 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,510 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,513 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,519 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,524 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,533 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,537 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,544 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,549 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,557 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,568 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,842 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:39,853 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 16:01:39,855 - Going to remove the following attributes: ['ID']\n",
      "DEBUG: 2020-09-24 16:01:42,553 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:42,703 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:43,348 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:43,367 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:43,790 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:43,807 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:44,353 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:44,365 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:44,890 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 16:01:44,985 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 16:01:46,296 - 633 promises submitted to pool\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-26101e0a5d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpromise\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpromises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Finished promises: {len(data)}/{len(promises)} ({len(data)/len(promises)*100:.2f}%)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool(processes=n_procs) as pool:\n",
    "    \n",
    "    start_at = 4368\n",
    "    \n",
    "    output_file = f\"metrics_{int(time.time())}.dat\"\n",
    "    logging.info(f\"Output to {output_file}\")\n",
    "    \n",
    "    promises = []\n",
    "    counter = 0\n",
    "    for model_id in MODELS.keys():\n",
    "        for task_id in TASKS:\n",
    "            tmp, counter = get_cv_metrics_for_model_and_task(model_id, task_id, pool, 10, counter, start_at)\n",
    "            promises.extend(tmp)\n",
    "            \n",
    "    logging.info(f\"{len(promises)} promises submitted to pool\")\n",
    "            \n",
    "    data = []\n",
    "    for promise in promises:\n",
    "        data.append(promise.get())\n",
    "        logging.info(f\"Finished promises: {len(data)}/{len(promises)} ({len(data)/len(promises)*100:.2f}%)\")\n",
    "        df = pandas.DataFrame(data)\n",
    "        dump(df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load(\"/home/maximl/Data/Experiment_data/results/riverrel/metrics_1600887779.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby([\"model_id\", \"task_id\", \"repeat\"]).aggregate(\"mean\").drop(columns=[\"fold\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longform(df, cols=None, subject_cols=None):\n",
    "    dfs = []\n",
    "    \n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    \n",
    "    for col in cols:\n",
    "        tmp_df = pandas.DataFrame(dict(        \n",
    "            value=df[col], \n",
    "            metric=col,   \n",
    "        ))\n",
    "        for col2 in set(df.columns) - set(cols):\n",
    "            tmp_df[col2] = df[col2]\n",
    "            \n",
    "        if subject_cols is not None:\n",
    "            tmp_df[\"subject\"] = df[subject_cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "            \n",
    "        dfs.append(tmp_df)\n",
    "        \n",
    "    return pandas.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = get_longform(grouped_df, grouped_df.columns[3:], [\"model_id\", \"task_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin([\"accuracy\", \"balanced_accuracy\", \"f1\"])], x=\"model_id\", y=\"value\", col=\"metric\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin([\"accuracy\", \"balanced_accuracy\", \"f1\"])], x=\"task_id\", y=\"value\", col=\"metric\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.displot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], x=\"value\", col=\"metric\", rug=True, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.boxplot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], y=\"value\", x=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lineplot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], y=\"value\", x=\"metric\", hue=\"subject\", err_style=\"bars\", palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled datasets + model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = grouped_df.loc[grouped_df[\"repeat\"] == 0, [\"ece\", \"ece_balanced\", \"peace\"]].values\n",
    "scipy.stats.friedmanchisquare(data[0], data[1], data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_data = get_longform(grouped_df.loc[grouped_df[\"repeat\"] == 0, [\"ece\", \"ece_balanced\", \"peace\"]])\n",
    "sp.posthoc_conover(long_data, val_col=\"value\", group_col=\"metric\", p_adjust=\"holm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model_df in grouped_df.groupby(\"model_id\"):\n",
    "    data = model_df.loc[:, [\"ece\", \"ece_balanced\", \"peace\"]]\n",
    "    test = scipy.stats.friedmanchisquare(data.iloc[:, 0], data.iloc[:, 1], data.iloc[:, 2])\n",
    "    print(idx)\n",
    "    print(test)\n",
    "    if test.pvalue < 0.05:\n",
    "        long_data = get_longform(data)\n",
    "        print(sp.posthoc_conover(long_data, val_col=\"value\", group_col=\"metric\", p_adjust=\"holm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], x=\"metric\", y=\"value\", col=\"model_id\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
