{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook compare_metrics_over_datasets.ipynb to python\n",
      "[NbConvertApp] Writing 7797 bytes to compare_metrics_over_datasets.py\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "!jupyter nbconvert --RegexRemovePreprocessor.patterns=\"['# ?noscript']\" --TemplateExporter.exclude_markdown=True --to python compare_metrics_over_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import openml\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.base\n",
    "import sklearn.utils\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.neural_network\n",
    "\n",
    "from ridgereliability import plots, metrics\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "import logging\n",
    "import time\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_ipython().__class__.__name__ != 'ZMQInteractiveShell':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n-processes\", type=int, required=True)\n",
    "    n_procs = parser.parse_args().n_processes\n",
    "else:\n",
    "    n_procs = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare metrics over datasets\n",
    "\n",
    "In this notebook we collect calibration estimates for several models and datasets. The estimates are computed with the Expected Calibration Error (ECE), Balanced-ECE, and Posterior Expected Accuracy-based Calibration Error (PEACE).\n",
    "\n",
    "Tested models:\n",
    "- Random Forest\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Gaussian Naive Bayes\n",
    "- MLP\n",
    "\n",
    "Tested datasets (from Open ML):\n",
    "- [eeg-eye-state](https://www.openml.org/d/1471), task [9983](https://www.openml.org/t/9983)\n",
    "- [Phoneme](https://www.openml.org/d/1489), task [9952](https://www.openml.org/t/9952)\n",
    "- [mozilla4](https://www.openml.org/d/1046), task [3899](https://www.openml.org/t/3899)\n",
    "- [electricity](https://www.openml.org/d/151), task [219](https://www.openml.org/t/219)\n",
    "- [Magic Telescope](https://www.openml.org/d/1120), task [3954](https://www.openml.org/t/3954)\n",
    "- [artificial characters](https://www.openml.org/d/1459), task [14964](https://www.openml.org/t/14964)\n",
    "- [pendigits](https://www.openml.org/d/32), task [32](https://www.openml.org/t/32)\n",
    "- [letter](https://www.openml.org/d/6), task [6](https://www.openml.org/t/6)\n",
    "- [JapaneseVowels](https://www.openml.org/d/375), task [3510](https://www.openml.org/t/3510)\n",
    "- [glass](https://www.openml.org/d/41), task [40](https://www.openml.org/t/40)\n",
    "- [micro-mass](https://www.openml.org/d/1515), task [9950](https://www.openml.org/t/9950)\n",
    "- [vehicle](https://www.openml.org/d/54), task [53](https://www.openml.org/t/53)\n",
    "- [synthetic-control](https://www.openml.org/d/377), task [3512](https://www.openml.org/t/3512)\n",
    "- [mfeat-factors](https://www.openml.org/d/12), task [12](https://www.openml.org/t/12)\n",
    "- [OVA_breast](https://www.openml.org/d/1128), task [3962](https://www.openml.org/d/1128)\n",
    "- [sonar](https://www.openml.org/d/40), task [39](https://www.openml.org/t/39)\n",
    "- [visualizing_livestock](https://www.openml.org/d/685), task [3577](https://www.openml.org/t/3577)\n",
    "- [spectrometer](https://www.openml.org/d/313), task [145682](https://www.openml.org/t/145682)\n",
    "- [disclosure_z](https://www.openml.org/d/931), task [3794](https://www.openml.org/t/3794)\n",
    "- [mfeat-pixel](https://www.openml.org/d/40979), task [146824](https://www.openml.org/t/146824)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "First of all, we need to be able to load and preprocess multiple datasets appropriately to prepare it for classification. \n",
    "To this end we implement a function that will load a dataset and classification task from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [9983, 9952, 3899, 219, 3954, 14964, 32, 6, 3510, 40, 9950, 53, 3512, 12, 3962, 39, 3577, 145682, 3794, 146824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openml_task(task_id):\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    X, y = task.get_X_and_y(\"dataframe\")\n",
    "    \n",
    "    X_enc = sklearn.preprocessing.OrdinalEncoder().fit_transform(X.values)\n",
    "    X = pandas.DataFrame(X_enc, columns=X.columns, index=X.index)\n",
    "    \n",
    "    n_repeats, n_folds, n_samples = task.get_split_dimensions()\n",
    "\n",
    "    folds = numpy.empty((len(X)), dtype=int)\n",
    "    for fold_idx in range(n_folds):\n",
    "        _, test_indices = task.get_train_test_split_indices(\n",
    "            repeat=0,\n",
    "            fold=fold_idx,\n",
    "            sample=0,\n",
    "        )\n",
    "        \n",
    "        folds[test_indices] = fold_idx\n",
    "        \n",
    "    splitter = sklearn.model_selection.PredefinedSplit(folds)\n",
    "                \n",
    "    return X, y, splitter        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: 2020-09-24 17:05:22,807 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:22,811 - Data pickle file already exists and is up to date.\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "X, y, splitter = load_openml_task(9952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(541,)\n",
      "(541,)\n",
      "(541,)\n",
      "(541,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "for train_idx, test_idx in splitter.split():\n",
    "    print(test_idx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting calibration metrics\n",
    "\n",
    "For each dataset we fit and evaluate multiple models. We record calibration metrics (ECE, ECE-balanced, PEACE), and some performance metrics (balanced accuracy, F1-score, accuracy). To this end we implement a function that runs this procedure for one model on one OpenML task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"rf\": sklearn.ensemble.RandomForestClassifier(),\n",
    "    \"svm\": sklearn.svm.SVC(probability=True),\n",
    "    \"logreg\": sklearn.linear_model.LogisticRegression(max_iter=500),\n",
    "    \"nb\": sklearn.naive_bayes.GaussianNB(),\n",
    "    \"mlp\": sklearn.neural_network.MLPClassifier(max_iter=500)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_metrics_for_model(row, Xt, yt, Xv, yv):\n",
    "    # get and fit fresh model\n",
    "    model = sklearn.base.clone(MODELS[row[\"model_id\"]])\n",
    "    model.fit(Xt, yt)\n",
    "\n",
    "    # compute metrics on test data\n",
    "    y_probs = model.predict_proba(Xv)\n",
    "    y_probs_max = y_probs.max(axis=1)\n",
    "    y_preds = model.predict(Xv)\n",
    "    y_test = yv\n",
    "\n",
    "    bins = 15\n",
    "    row.update({\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(y_test, y_preds),\n",
    "        \"balanced_accuracy\": sklearn.metrics.balanced_accuracy_score(y_test, y_preds),\n",
    "        \"f1\": sklearn.metrics.f1_score(y_test, y_preds, average=\"weighted\"),\n",
    "        'ece': metrics.ece(y_probs_max, y_preds, y_test, bins=bins),\n",
    "        'ece_balanced': metrics.ece(y_probs_max, y_preds, y_test, balanced=True, bins=bins),\n",
    "        'peace': metrics.peace(y_probs_max, y_preds, y_test, bins=bins),\n",
    "        'class_wise_ece': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.ece, bins=bins),\n",
    "        'class_wise_peace': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.peace, bins=bins)\n",
    "    })\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_metrics_for_model_and_task(model_id, task_id, pool, n_repeats, counter, start_at):\n",
    "    \n",
    "    X, y, splitter = load_openml_task(task_id) # repeated runs will use cached data\n",
    "    \n",
    "    promises = []\n",
    "    for i, (train_idx, test_idx) in enumerate(splitter.split()):\n",
    "        for j in range(n_repeats):\n",
    "            counter += 1\n",
    "            if counter < start_at:\n",
    "                continue\n",
    "            \n",
    "            row = {\n",
    "                \"fold\": i,\n",
    "                \"repeat\": j,\n",
    "                \"model_id\": model_id,\n",
    "                \"task_id\": task_id,\n",
    "            }\n",
    "\n",
    "            # split data\n",
    "            Xt, yt = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            Xv, yv = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "            promise = pool.apply_async(\n",
    "                get_fold_metrics_for_model,\n",
    "                (row, Xt, yt, Xv, yv)\n",
    "            )\n",
    "            promises.append(promise)\n",
    "        \n",
    "    return promises, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection over all datasets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2020-09-24 17:05:22,893 - Output to metrics_1600959922.dat\n",
      "DEBUG: 2020-09-24 17:05:22,899 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:22,905 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:22,971 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:22,982 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,033 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,043 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,096 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,115 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,416 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,437 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 17:05:23,439 - Going to remove the following attributes: ['ID']\n",
      "DEBUG: 2020-09-24 17:05:23,564 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,584 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,650 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,661 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,751 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,766 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,872 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,881 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,964 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:23,972 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:24,117 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:24,238 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:24,560 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:24,571 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:24,668 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:24,708 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 17:05:24,711 - Going to remove the following attributes: ['index']\n",
      "DEBUG: 2020-09-24 17:05:24,944 - Data pickle file already exists and is up to date.\n",
      "DEBUG: 2020-09-24 17:05:24,983 - Data pickle file already exists and is up to date.\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "DEBUG: 2020-09-24 17:05:26,528 - Data pickle file already exists and is up to date.\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "DEBUG: 2020-09-24 17:05:27,645 - Data pickle file already exists and is up to date.\n",
      "INFO: 2020-09-24 17:05:27,750 - Going to remove the following attributes: ['ID_REF']\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-13e809f10802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTASKS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cv_metrics_for_model_and_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mpromises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c4debd87ea98>\u001b[0m in \u001b[0;36mget_cv_metrics_for_model_and_task\u001b[0;34m(model_id, task_id, pool, n_repeats, counter, start_at)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_cv_metrics_for_model_and_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_openml_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# repeated runs will use cached data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpromises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-732ad3844104>\u001b[0m in \u001b[0;36mload_openml_task\u001b[0;34m(task_id)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_X_and_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataframe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mTransformed\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \"\"\"\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mXi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             diff, valid_mask = _encode_check_unknown(Xi, self.categories_[i],\n\u001b[0m\u001b[1;32m    118\u001b[0m                                                      return_mask=True)\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode_check_unknown\u001b[0;34m(values, uniques, return_mask)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0munique_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool(processes=n_procs) as pool:\n",
    "    \n",
    "    start_at = 0\n",
    "    \n",
    "    output_file = f\"metrics_{int(time.time())}.dat\"\n",
    "    logging.info(f\"Output to {output_file}\")\n",
    "    \n",
    "    promises = []\n",
    "    counter = 0\n",
    "    for model_id in MODELS.keys():\n",
    "        for task_id in TASKS:\n",
    "            tmp, counter = get_cv_metrics_for_model_and_task(model_id, task_id, pool, 1, counter, start_at)\n",
    "            promises.extend(tmp)\n",
    "            \n",
    "    logging.info(f\"{len(promises)} promises submitted to pool\")\n",
    "            \n",
    "    data = []\n",
    "    for promise in promises:\n",
    "        data.append(promise.get())\n",
    "        logging.info(f\"Finished promises: {len(data)}/{len(promises)} ({len(data)/len(promises)*100:.2f}%)\")\n",
    "        df = pandas.DataFrame(data)\n",
    "        dump(df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_ipython().__class__.__name__ != 'ZMQInteractiveShell':\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load(\"/home/maximl/Data/Experiment_data/results/riverrel/metrics_1600940535.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load(\"metrics_1600957213.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby([\"model_id\", \"task_id\", \"repeat\"]).aggregate(\"mean\").drop(columns=[\"fold\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longform(df, cols=None, subject_cols=None):\n",
    "    dfs = []\n",
    "    \n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    \n",
    "    for col in cols:\n",
    "        tmp_df = pandas.DataFrame(dict(        \n",
    "            value=df[col], \n",
    "            metric=col,   \n",
    "        ))\n",
    "        for col2 in set(df.columns) - set(cols):\n",
    "            tmp_df[col2] = df[col2]\n",
    "            \n",
    "        if subject_cols is not None:\n",
    "            tmp_df[\"subject\"] = df[subject_cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "            \n",
    "        dfs.append(tmp_df)\n",
    "        \n",
    "    return pandas.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = get_longform(grouped_df, grouped_df.columns[3:], [\"model_id\", \"task_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin([\"accuracy\", \"balanced_accuracy\", \"f1\"])], x=\"model_id\", y=\"value\", col=\"metric\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin([\"accuracy\", \"balanced_accuracy\", \"f1\"])], x=\"task_id\", y=\"value\", col=\"metric\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.displot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], x=\"value\", col=\"metric\", rug=True, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.boxplot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], y=\"value\", x=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lineplot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], y=\"value\", x=\"metric\", hue=\"subject\", err_style=\"bars\", palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled datasets + model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = grouped_df.loc[grouped_df[\"repeat\"] == 0, [\"ece\", \"ece_balanced\", \"peace\"]].values\n",
    "scipy.stats.friedmanchisquare(data[0], data[1], data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_data = get_longform(grouped_df.loc[grouped_df[\"repeat\"] == 0, [\"ece\", \"ece_balanced\", \"peace\"]])\n",
    "sp.posthoc_conover(long_data, val_col=\"value\", group_col=\"metric\", p_adjust=\"holm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model_df in grouped_df.groupby(\"model_id\"):\n",
    "    data = model_df.loc[:, [\"ece\", \"ece_balanced\", \"peace\"]]\n",
    "    test = scipy.stats.friedmanchisquare(data.iloc[:, 0], data.iloc[:, 1], data.iloc[:, 2])\n",
    "    print(idx)\n",
    "    print(test)\n",
    "    if test.pvalue < 0.05:\n",
    "        long_data = get_longform(data)\n",
    "        print(sp.posthoc_conover(long_data, val_col=\"value\", group_col=\"metric\", p_adjust=\"holm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=long_df[long_df[\"metric\"].isin([\"ece\", \"ece_balanced\", \"peace\"])], x=\"metric\", y=\"value\", col=\"model_id\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
