{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook compare_calibrations.ipynb to python\n",
      "[NbConvertApp] Writing 11576 bytes to compare_calibrations.py\n"
     ]
    }
   ],
   "source": [
    "# noscript\n",
    "!jupyter nbconvert --RegexRemovePreprocessor.patterns=\"['# ?noscript']\" --TemplateExporter.exclude_markdown=True --to python compare_calibrations.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from ridgereliability import metrics, plots\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "from dirichlet.calib import tempscaling, vectorscaling\n",
    "from dirichlet import DirichletCalibrator\n",
    "from sklearn.base import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from sklearn.utils import check_X_y, indexable, column_or_1d\n",
    "from sklearn.utils.validation import check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class F(Enum):\n",
    "    TRAIN = 1\n",
    "    EVAL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = F.TRAIN\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --out OUT\n",
      "ipykernel_launcher.py: error: the following arguments are required: --out\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximl/.conda/envs/ml/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if f is F.TRAIN:\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--out\", type=str, required=True)\n",
    "    output_dir = parser.parse_args().out\n",
    "elif f is F.EVAL:\n",
    "    output_dir = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare calibration methods on CIFAR-10\n",
    "\n",
    "We hypothesize that the PEACE metric and posterior reliability diagram will be better measures for finding an optimal post-hoc calibration method. Since PEACE is computed using full posterior balanced accuracy distributions, it takes into account the variance of probability estimates in each bin. This means using PEACE we can select a calibration method that produces probability estimates with high certainty, something we can not check with ECE.\n",
    "\n",
    "Here, we test this hypothesis by calibrating a DenseNet trained on CIFAR-10 with temperature scaling and Platt scaling. We begin by loading a baseline model and evaluating it. Then, we do the same for calibrated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f is F.TRAIN:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                print(gpu)\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "            \n",
    "    dev = [d for d in tf.config.experimental.list_logical_devices() if d.device_type==\"GPU\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noscript\n",
    "if f is F.EVAL:\n",
    "    from collections import namedtuple\n",
    "    device = namedtuple(\"device\", [\"name\"])\n",
    "    dev = device(name=\"/CPU:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "\n",
    "x_test = (x_test - mean) / (std + 1e-7)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "x_train = (x_train - mean) / (std + 1e-7)\n",
    "y_train = tf.keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train baseline uncalibrated DenseNet\n",
    "\n",
    "We will first train two models:  \n",
    "- One is trained on the full train set, and will be used as a baseline model.\n",
    "- The other is trained on the smaller version of the full train set from which validation data is held-out for calibration. On this model we will apply post-hoc calibration methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f is F.TRAIN:\n",
    "    with tf.device(dev.name):\n",
    "        model = tf.keras.applications.DenseNet121(\n",
    "            include_top=True,\n",
    "            weights=None,\n",
    "            classes=10,\n",
    "            input_shape=[32, 32, 3]\n",
    "        )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.save_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the full train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f is F.TRAIN:\n",
    "    with tf.device(dev.name):\n",
    "        datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                    samplewise_center=False,  # set each sample mean to 0\n",
    "                    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                    samplewise_std_normalization=False,  # divide each input by its std\n",
    "                    zca_whitening=False,  # apply ZCA whitening\n",
    "                    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                    horizontal_flip=True,  # randomly flip images\n",
    "                    vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f is F.TRAIN:\n",
    "    model.fit(\n",
    "        datagen.fit(x_train).flow(x_train, y_train, batch_size=128),\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        steps_per_epoch=np.ceil(len(x_train)/128)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noscript\n",
    "if f is F.EVAL:\n",
    "    with tf.device(dev.name):\n",
    "        models[\"baseline\"] = tf.keras.models.load_model('models/baseline.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the small train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_s, x_val, y_train_s, y_val = sklearn.model_selection.train_test_split(x_train, y_train.argmax(axis=1), test_size=0.25)\n",
    "\n",
    "mean = x_train_s.mean()\n",
    "std = x_train_s.std()\n",
    "\n",
    "x_train_s = (x_train_s - mean) / (std + 1e-7)\n",
    "y_train_s = tf.keras.utils.to_categorical(y_train_s)\n",
    "\n",
    "x_val = (x_val - mean) / (std + 1e-7)\n",
    "y_val = tf.keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f is F.TRAIN:\n",
    "    model.load_weights(\"models/model.h5\")\n",
    "\n",
    "    model.fit(\n",
    "        datagen.fit(x_train_s).flow(x_train, y_train, batch_size=128),\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        steps_per_epoch=np.ceil(len(x_train)/128)\n",
    "    )\n",
    "\n",
    "    model.save('models/cifar10_densenet_baseline_s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noscript\n",
    "if f is F.EVAL:\n",
    "    with tf.device(dev.name):\n",
    "        models[\"baseline_s\"] = tf.keras.models.load_model('models/cifar10_densenet_baseline_s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f is F.TRAIN:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test, keras=False, bins=15):\n",
    "    \n",
    "    if keras:\n",
    "        y_probs = model.predict(x_test, batch_size=128, verbose=0)\n",
    "    else:\n",
    "        y_probs = model.predict_proba(x_test)\n",
    "    y_probs_max = y_probs.max(axis=1)\n",
    "    y_preds = y_probs.argmax(axis=1)\n",
    "    y_test = y_test.argmax(axis=1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(y_test, y_preds),\n",
    "        \"balanced_accuracy\": sklearn.metrics.balanced_accuracy_score(y_test, y_preds),\n",
    "        'ece': metrics.ece(y_probs_max, y_preds, y_test, bins=bins),\n",
    "        'ece_balanced': metrics.ece(y_probs_max, y_preds, y_test, balanced=True, bins=bins),\n",
    "        'peace': metrics.peace(y_probs_max, y_preds, y_test, bins=bins),\n",
    "        'class_wise_ece': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.ece, bins=bins),\n",
    "        'class_wise_ece_balanced': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.ece, balanced=True, bins=bins),\n",
    "        'class_wise_peace': metrics.class_wise_error(y_probs, y_preds, y_test, metrics.peace, bins=bins)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_metrics[\"baseline\"] = evaluate_model(models[\"baseline\"], x_test, y_test, keras=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit calibration methods on baseline (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibratedModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator=None, calibrator=None):\n",
    "        ''' Initialize a Calibrated model (classifier + calibrator)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator : string\n",
    "            Name of the classifier\n",
    "        method : string\n",
    "            Name of the calibrator\n",
    "        '''\n",
    "        self.base_estimator = base_estimator\n",
    "        self.calibrator = calibrator\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None, *args, **kwargs):\n",
    "        \"\"\"Fit the calibrated model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        y : array-like, shape (n_samples, n_classes)\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        scores = self.base_estimator.predict(X, batch_size=128, verbose=1)\n",
    "\n",
    "        if X_val is not None:\n",
    "            X_val, y_val = indexable(X_val, y_val)\n",
    "            # TODO add scores of validation\n",
    "            scores_val = self.base_estimator.predict(X_val, batch_size=128, verbose=1)\n",
    "        else:\n",
    "            scores_val = None\n",
    "\n",
    "        self.calibrator.fit(scores, y, X_val=scores_val, y_val=y_val, *args, **kwargs)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Posterior probabilities of classification\n",
    "\n",
    "        This function returns posterior probabilities of classification\n",
    "        according to each class on an array of test vectors X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples, n_classes)\n",
    "            The predicted probas. Can be exact zeros.\n",
    "        \"\"\"\n",
    "\n",
    "        scores = self.base_estimator.predict(X, batch_size=256, verbose=1)\n",
    "\n",
    "        predictions = self.calibrator.predict_proba(scores)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, X, *args):\n",
    "        \"\"\"Predict the target of new samples. Can be different from the\n",
    "        prediction of the uncalibrated classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples,)\n",
    "            The predicted class.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"calibrator\"])\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lambda_list = [1, 0.1 , 0.01, 0.001, 0.0001]\n",
    "reg_mu_list = [1, 0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrators = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator = tempscaling.TemperatureScaling(reg_lambda_list=reg_lambda_list, reg_mu_list=reg_mu_list, logit_constant=0.0)\n",
    "models[\"temperature\"] = CalibratedModel(models[\"baseline_s\"], calibrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"temperature\"].fit(x_train_s, y_train_s.argmax(axis=1), X_val=x_val, y_val=y_val.argmax(axis=1), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_metrics[\"temperature\"] = evaluate_model(models[\"temperature\"], x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Dirichlet calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator = DirichletCalibrator(matrix_type=\"full\", l2=0.1)\n",
    "models[\"dirichlet\"] = CalibratedModel(models[\"baseline_s\"], calibrator)\n",
    "models[\"dirichlet\"].fit(x_train_s, y_train_s.argmax(axis=1), X_val=x_val, y_val=y_val.argmax(axis=1), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_metrics[\"dirichlet\"] = evaluate_model(models[\"dirichlet\"], x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Vector scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator = vectorscaling.VectorScaling(reg_lambda_list=reg_lambda_list, reg_mu_list=reg_mu_list)\n",
    "models[\"vector\"] = CalibratedModel(models[\"baseline_s\"], calibrator)\n",
    "models[\"vector\"].fit(x_train_s, y_train_s.argmax(axis=1), X_val=x_val, y_val=y_val.argmax(axis=1), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_metrics[\"vector\"] = evaluate_model(models[\"vector\"], x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save calibrated models and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, model in models:\n",
    "    if type(model) is tf.python.keras.engine.functional.Functional:\n",
    "        fname = os.path.join(output_dir, f\"{k}.h5\")\n",
    "        model.save(fname)\n",
    "    else:\n",
    "        fname = os.path.join(output_dir, f\"{k}.h5\")\n",
    "        model.calibrator = None\n",
    "        dump(model, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(calibration_metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(os.path.join(output_dir, \"calibration_metrics.dat\"), df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f is F.TRAIN:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"ece\", \"peace\", \"class_wise_ece\", \"class_wise_peace\"]].plot.bar().legend(loc='center left',bbox_to_anchor=(1.0, 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[df[\"peace\"].argmin()], df[\"peace\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[df[\"class_wise_peace\"].argmin()], df[\"class_wise_peace\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[df[\"ece\"].argmin()], df[\"ece\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[df[\"class_wise_ece\"].argmin()], df[\"class_wise_ece\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(models), figsize=(len(models)*3, 3), constrained_layout=True)\n",
    "for ax, (k, model) in zip(axes, models.items()):\n",
    "    \n",
    "    ax.set_title(k)\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_probs = model.predict_proba(x_test)\n",
    "    else:\n",
    "        y_probs = model.predict(x_test, batch_size=256, verbose=1)\n",
    "    y_probs_max = y_probs.max(axis=1)\n",
    "    y_preds = y_probs.argmax(axis=1)\n",
    "    \n",
    "    plots.posterior_reliability_diagram(y_probs_max, y_preds, y_test.argmax(axis=1), bins=15, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 10, figsize=(25, 6), sharex=True, sharey=True, constrained_layout=True)\n",
    "for row, (k, model) in zip(axes, {k:v for k,v in models.items() if k in [\"temperature\", \"vector\"]}.items()):\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_probs = model.predict_proba(x_test)\n",
    "    else:\n",
    "        y_probs = model.predict(x_test, batch_size=256, verbose=1)\n",
    "    y_preds = y_probs.argmax(axis=1)\n",
    "\n",
    "    plots.class_wise_posterior_reliability_diagram(y_probs, y_preds, y_test.argmax(axis=1), bins=10, axes=row, metric=metrics.peace)\n",
    "    \n",
    "    row[0].set_ylabel(k + \"\\n\" + row[0].get_ylabel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
